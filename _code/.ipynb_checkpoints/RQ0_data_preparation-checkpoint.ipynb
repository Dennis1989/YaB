{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177052e-ec88-4ec4-b05c-14d08bd9b782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "new_path = os.path.split(os.getcwd())[0]\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "from emoji import demojize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf07dc9-c27b-4dfb-b3a8-d64cf0794cd5",
   "metadata": {},
   "source": [
    "### convert monthly api output (.jsonl) to dataframes (.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192fd22-769f-44ea-8ea8-e0309ebf8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_total = re.compile('[Bb][Oo][Tt]')\n",
    "re_mentions = re.compile('@{1}[a-zA-Z0-9_]*[Bb][Oo][Tt][a-zA-Z0-9_]*')\n",
    "\n",
    "re_sub_mentions = re.compile('\\s{0,1}@{1}[a-zA-Z0-9_]+\\s{0,1}')\n",
    "re_sub_urls = re.compile('\\s{0,1}https://t.co/[a-zA-Z0-9]+\\s{0,1}')\n",
    "\n",
    "re_bot_lax = re.compile('you are .*bot|you\\'re .*bot')\n",
    "re_bot_strict = re.compile('you are a [a-z]*bot|you\\'re a [a-z]*bot')\n",
    "\n",
    "def bot_in_text_t2(row):\n",
    "    bot_total = re_total.findall(row['t2_text']) # total occurrences of 'bot' in t2_text \n",
    "    bot_mentions = re_mentions.findall(row['t2_text']) # occurrences of 'bot' in t2_text mentions\n",
    "    if not bot_total: # 'bot' not in t2_text at all\n",
    "        return (0,0)\n",
    "    else:\n",
    "        if len(bot_total) == len(bot_mentions): # 'bot' only in mentions\n",
    "            return (0,1)\n",
    "        else:\n",
    "            if len(bot_mentions) == 0: # 'bot' only in text, not in mentions\n",
    "                return (1,0)\n",
    "            else: # 'bot' both in text and in mentions\n",
    "                return (1,1)\n",
    "            \n",
    "def get_monologue(row):\n",
    "    return 1 if row['t1_author_id'] == row['t2_author_id'] else 0\n",
    "\n",
    "def clean_text_t1(row):\n",
    "    try:\n",
    "        text = re_sub_mentions.sub(' ',row['t1_text'])\n",
    "        text = re_sub_urls.sub(' ',text)\n",
    "        return text.strip()\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def clean_text_t2(row):\n",
    "    try:\n",
    "        text = re_sub_mentions.sub(' ',row['t2_text'])\n",
    "        text = re_sub_urls.sub(' ',text)\n",
    "        return text.strip()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def match_bot_pattern_lax_t1(row):\n",
    "    if row['t1_text_cleaned'] == None:\n",
    "        return 0\n",
    "    elif re_bot_lax.findall(row['t1_text_cleaned'].lower()):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def match_bot_pattern_strict_t1(row):\n",
    "    if row['t1_text_cleaned'] == None:\n",
    "        return 0\n",
    "    elif re_bot_strict.findall(row['t1_text_cleaned'].lower()):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def match_bot_pattern_lax_t2(row):\n",
    "    if row['t2_text_cleaned'] == None:\n",
    "        return 0\n",
    "    elif re_bot_lax.findall(row['t2_text_cleaned'].lower()):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def match_bot_pattern_strict_t2(row):\n",
    "    if row['t2_text_cleaned'] == None:\n",
    "        return 0\n",
    "    elif re_bot_strict.findall(row['t2_text_cleaned'].lower()):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def load_and_process(filename):\n",
    "    \n",
    "    tweets = []\n",
    "    with open(filename,'r') as file: \n",
    "        for line in file:\n",
    "            try:\n",
    "                tweets.append(json.loads(line))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    tweet_fields = ['conversation_id','id','created_at','geo','in_reply_to_user_id','public_metrics','source','text']\n",
    "    user_fields = ['created_at','description','id','location','name','profile_image_url','public_metrics','url','username','verified']\n",
    "    fields = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "\n",
    "        fields_ = []\n",
    "\n",
    "        if tweet['t1'] != None:\n",
    "            tf_1 = [tweet['t1'][tf] if (tf in tweet['t1'].keys()) else None for tf in tweet_fields]\n",
    "        else:\n",
    "            tf_1 = [None for i in range(len(tweet_fields))]\n",
    "        fields_.extend(tf_1)\n",
    "\n",
    "        if tweet['t2'] != None:\n",
    "            tf_2 = [tweet['t2'][tf] if (tf in tweet['t2'].keys()) else None for tf in tweet_fields]\n",
    "        else:\n",
    "            tf_2 = [None for i in range(len(tweet_fields))]\n",
    "        fields_.extend(tf_2)\n",
    "\n",
    "        if tweet['t1_author'] != None:\n",
    "            uf_1 = [tweet['t1_author'][uf] if (uf in tweet['t1_author'].keys()) else None for uf in user_fields]\n",
    "        else:\n",
    "            uf_1 = [None for i in range(len(user_fields))]\n",
    "        fields_.extend(uf_1)\n",
    "\n",
    "        if tweet['t2_author'] != None:\n",
    "            uf_2 = [tweet['t2_author'][uf] if (uf in tweet['t2_author'].keys()) else None for uf in user_fields]\n",
    "        else:\n",
    "            uf_2 = [None for i in range(len(user_fields))]\n",
    "        fields_.extend(uf_2)\n",
    "\n",
    "        fields.append(fields_)\n",
    "\n",
    "    pd_tweets = pd.DataFrame(fields, columns=['t1_'+tf for tf in tweet_fields]+['t2_'+tf for tf in tweet_fields]+['t1_author_'+uf for uf in user_fields]+['t2_author_'+uf for uf in user_fields])\n",
    "    pd_tweets = pd_tweets[~pd_tweets['t2_id'].isnull()]\n",
    "    pd_tweets = pd_tweets.drop_duplicates('t2_id')\n",
    "    pd_tweets = pd_tweets.replace('',None)\n",
    "    pd_tweets = pd_tweets.sort_values('t2_created_at')\n",
    "    pd_tweets = pd_tweets.reset_index(drop=True)\n",
    "    \n",
    "    return pd_tweets\n",
    "\n",
    "def augment(pd_tweets):\n",
    "    pd_tweets['filter_dummy'] = pd_tweets.apply(bot_in_text_t2, axis=1)\n",
    "    pd_tweets['filter_bot_in_text'] = [t[0] for t in pd_tweets['filter_dummy']]\n",
    "    pd_tweets['filter_bot_in_mentions'] = [t[1] for t in pd_tweets['filter_dummy']]\n",
    "    pd_tweets = pd_tweets.drop('filter_dummy', axis=1)\n",
    "    pd_tweets['filter_monologue'] = pd_tweets.apply(get_monologue, axis=1)\n",
    "    pd_tweets['t1_text_cleaned'] = pd_tweets.apply(clean_text_t1, axis=1)\n",
    "    pd_tweets['t2_text_cleaned'] = pd_tweets.apply(clean_text_t2, axis=1)\n",
    "\n",
    "    pd_tweets['filter_t1_bot_pattern_lax'] = pd_tweets.apply(match_bot_pattern_lax_t1, axis=1)\n",
    "    pd_tweets['filter_t1_bot_pattern_strict'] = pd_tweets.apply(match_bot_pattern_strict_t1, axis=1)\n",
    "    pd_tweets['filter_t2_bot_pattern_lax'] = pd_tweets.apply(match_bot_pattern_lax_t2, axis=1)\n",
    "    pd_tweets['filter_t2_bot_pattern_strict'] = pd_tweets.apply(match_bot_pattern_strict_t2, axis=1)\n",
    "\n",
    "    pd_tweets['t1_datetime'] = pd.to_datetime(pd_tweets['t1_created_at'])\n",
    "    pd_tweets['t2_datetime'] = pd.to_datetime(pd_tweets['t2_created_at'])\n",
    "\n",
    "    pd_tweets['t1_year'] = [t.year for t in pd_tweets['t1_datetime']]\n",
    "    pd_tweets['t1_month'] = [t.month for t in pd_tweets['t1_datetime']]\n",
    "    pd_tweets['t2_year'] = [t.year for t in pd_tweets['t2_datetime']]\n",
    "    pd_tweets['t2_month'] = [t.month for t in pd_tweets['t2_datetime']]\n",
    "    return pd_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9244b-d4e3-4e40-9777-de64607d4cd7",
   "metadata": {},
   "source": [
    "conversion and augmentation of monthly tweet files from .jsonl to .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c61c5a-3898-468f-84bd-9abeb6837250",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in os.listdir(os.path.join(new_path,'_data','_raw')):\n",
    "    print(year)\n",
    "    if year.startswith('.'):\n",
    "          continue\n",
    "    for month in [i+1 for i in range(12)]:\n",
    "        if os.path.exists(os.path.join(new_path,'_data','_raw',year,f'{year}_{str(month).zfill(2)}.pkl')):\n",
    "            continue\n",
    "        elif not os.path.exists(os.path.join(new_path,'_data','_raw',year,f'{year}_{str(month).zfill(2)}.jsonl')):\n",
    "            continue\n",
    "        else:\n",
    "            pd_tweets = load_and_process(os.path.join(new_path,'_data','_raw',year,f'{year}_{str(month).zfill(2)}.jsonl'))\n",
    "            pd_tweets = augment(pd_tweets)\n",
    "            pd_tweets.to_pickle(os.path.join(new_path,'_data','_raw',year,f'{year}_{str(month).zfill(2)}.pkl'))\n",
    "            print(f'Created .pkl for {year}_{str(month).zfill(2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed74792-f919-4212-9d00-5b7b609b950d",
   "metadata": {},
   "source": [
    "### assemble monthly dataframes into datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63b289-18fe-4598-8645-44df6de5b4ca",
   "metadata": {},
   "source": [
    "#### bot_direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a6780-f6f4-44a7-84f6-9aa4daa8ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_direct = pd.DataFrame()\n",
    "\n",
    "for year in os.listdir(os.path.join(new_path,'_data','_raw')):\n",
    "    if year.startswith('.'):\n",
    "        continue\n",
    "    for month in [i+1 for i in range(12)]:\n",
    "        if os.path.exists(os.path.join(new_path,'_data','_raw',year,f'{year}_{str(month).zfill(2)}.pkl')):\n",
    "            pd_month = pd.read_pickle(os.path.join(new_path,'_data','_raw',year,f'{year}_{str(month).zfill(2)}.pkl'))\n",
    "            pd_month_filtered = pd_month[(pd_month['filter_monologue']==0)&(pd_month['filter_t2_bot_pattern_strict']==1)&(pd_month['filter_bot_in_text']==1)]\n",
    "            bot_direct = pd.concat([bot_direct,pd_month_filtered])\n",
    "            n_total, n_bot_in_text, n_bot_pattern_lax, n_bot_pattern_strict = len(pd_month), len(pd_month[pd_month['filter_bot_in_text']==1]), len(pd_month[(pd_month['filter_monologue']==0)&(pd_month['filter_t2_bot_pattern_lax']==1)&(pd_month['filter_bot_in_text']==1)]), len(pd_month[(pd_month['filter_monologue']==0)&(pd_month['filter_t2_bot_pattern_strict']==1)&(pd_month['filter_bot_in_text']==1)])\n",
    "            with open(os.path.join(new_path,'accusations_overview.txt'), 'a') as f:\n",
    "                f.write(f'{year[1:]};{month};{n_total};{n_bot_in_text};{n_bot_pattern_lax};{n_bot_pattern_strict}\\n')\n",
    "                f.close()\n",
    "            print(f'Processed {year[1:]} {month}')\n",
    "            \n",
    "bot_direct = bot_direct.sort_values('t2_datetime', ascending=True)\n",
    "bot_direct = bot_direct.reset_index(drop=True)\n",
    "bot_direct.to_pickle(os.path.join(new_path,'_data','bot_direct.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c8572-b78b-44b8-939f-ddabc554ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bot_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8f097-77d5-4879-909e-c14167f6f382",
   "metadata": {},
   "source": [
    "#### bot_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad74ab-87ac-47e2-873d-0e240c37d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_all = pd.DataFrame()\n",
    "\n",
    "for year in os.listdir(os.path.join(new_path,'_data','_raw')):\n",
    "    if year.startswith('.'):\n",
    "        continue\n",
    "    for month in [i+1 for i in range(12)]:\n",
    "        if os.path.exists(os.path.join(new_path,'_data','_raw',year,f'{year}_{str(month).zfill(2)}.pkl')):\n",
    "            pd_month = pd.read_pickle(os.path.join(new_path,'_data','_raw',year,f'{year}_{str(month).zfill(2)}.pkl'))\n",
    "            pd_month_bottexts = pd_month[(pd_month['filter_bot_in_text']==1)]\n",
    "            pd_month_bottexts = pd_month_bottexts[['t1_id','t1_author_id','t1_author_username','t1_datetime','t1_text','t1_text_cleaned']+['t2_id','t2_author_id','t2_author_username','t2_datetime','t2_text','t2_text_cleaned']]\n",
    "            pd_month_bottexts = pd_month_bottexts.astype({'t1_author_id': 'category', 't1_author_username': 'category', 't2_id': int, 't2_author_id': 'category','t2_author_username': 'category'})\n",
    "            bot_all = pd.concat([bot_all,pd_month_bottexts])\n",
    "            print(f'Processed {year[1:]} {month}')\n",
    "            \n",
    "bot_all = bot_all.sort_values('t2_datetime', ascending=True)\n",
    "bot_all = bot_all.reset_index(drop=True)\n",
    "bot_all.to_pickle(os.path.join(new_path,'_data','bot_all.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1bf971-4460-469e-b950-ae807ebf1cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bot_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2efcdd6-e2e8-4992-baf5-d8bd5d937563",
   "metadata": {},
   "source": [
    "### preprocess bot_all for classifier training and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125aa0ec-8736-4110-8d12-eece95def47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(row):\n",
    "    re_mention = re.compile('\\s{0,1}@{1}[a-zA-Z0-9_]+\\s{0,1}')\n",
    "    text = row['t2_text']\n",
    "    t1_user = row['t1_author_username']\n",
    "    if t1_user == None:\n",
    "        text = re_mention.sub('@other_user ',text)\n",
    "        return 'To @unknown_user: ' + text\n",
    "    else:\n",
    "        text = text.replace('@'+t1_user,'t1_user ')\n",
    "        text = re_mention.sub('@other_user ',text)\n",
    "        text = text.replace('t1_user','@t1_user')\n",
    "        return 'To @t1_user: ' + text\n",
    "    \n",
    "def normalizeToken(token):\n",
    "    lowercased_token = token.lower()\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"@USER\"\n",
    "    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "        return \"HTTPURL\"\n",
    "    elif len(token) == 1:\n",
    "        return demojize(token)\n",
    "    else:\n",
    "        if token == \"’\":\n",
    "            return \"'\"\n",
    "        elif token == \"…\":\n",
    "            return \"...\"\n",
    "        else:\n",
    "            return token\n",
    "\n",
    "def normalizeTweet(row):\n",
    "    tweet = row['t2_text']\n",
    "    tokens = tokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"cannot \", \"can not \")\n",
    "        .replace(\"n't \", \" n't \")\n",
    "        .replace(\"n 't \", \" n't \")\n",
    "        .replace(\"ca n't\", \"can't\")\n",
    "        .replace(\"ai n't\", \"ain't\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"'m \", \" 'm \")\n",
    "        .replace(\"'re \", \" 're \")\n",
    "        .replace(\"'s \", \" 's \")\n",
    "        .replace(\"'ll \", \" 'll \")\n",
    "        .replace(\"'d \", \" 'd \")\n",
    "        .replace(\"'ve \", \" 've \")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\" p . m .\", \"  p.m.\")\n",
    "        .replace(\" p . m \", \" p.m \")\n",
    "        .replace(\" a . m .\", \" a.m.\")\n",
    "        .replace(\" a . m \", \" a.m \")\n",
    "    )\n",
    "\n",
    "    return \" \".join(normTweet.split())\n",
    "\n",
    "def normalizeTweet_t1(row):\n",
    "    try:\n",
    "        tweet = row['t1_text']\n",
    "        tokens = tokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
    "        normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "        normTweet = (\n",
    "            normTweet.replace(\"cannot \", \"can not \")\n",
    "            .replace(\"n't \", \" n't \")\n",
    "            .replace(\"n 't \", \" n't \")\n",
    "            .replace(\"ca n't\", \"can't\")\n",
    "            .replace(\"ai n't\", \"ain't\")\n",
    "        )\n",
    "        normTweet = (\n",
    "            normTweet.replace(\"'m \", \" 'm \")\n",
    "            .replace(\"'re \", \" 're \")\n",
    "            .replace(\"'s \", \" 's \")\n",
    "            .replace(\"'ll \", \" 'll \")\n",
    "            .replace(\"'d \", \" 'd \")\n",
    "            .replace(\"'ve \", \" 've \")\n",
    "        )\n",
    "        normTweet = (\n",
    "            normTweet.replace(\" p . m .\", \"  p.m.\")\n",
    "            .replace(\" p . m \", \" p.m \")\n",
    "            .replace(\" a . m .\", \" a.m.\")\n",
    "            .replace(\" a . m \", \" a.m \")\n",
    "        )\n",
    "\n",
    "        return \" \".join(normTweet.split())\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8cd1e-2eae-4170-96c5-4f1268c00618",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_all = pd.read_pickle(os.path.join(new_path,'_data','bot_all.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ddc9e-9a41-4555-803b-b66f5c4a975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_all['t2_text_processed'] = bot_all.apply(normalizeTweet, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d6878-827c-4a1d-b305-9233cb0e8d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_all.to_pickle(os.path.join(new_path,'_data','bot_all.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c39c451-d77a-4cba-9793-d1513a641c01",
   "metadata": {},
   "source": [
    "### deploy accusation classifier to bot_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd9ab1-18a4-4f36-8feb-b9926c90016e",
   "metadata": {},
   "source": [
    "load final accusation classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c12a127-9021-4519-9cec-8abd37713d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(os.path.join(new_path,'_model','accusation_classification_model'))\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(new_path,'_model','tokenizer'), model_max_length=512)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942839b-f2e6-4f8e-9554-829109408014",
   "metadata": {},
   "source": [
    "run model on bot_all\n",
    "- 6min 41s for 100,000 instances\n",
    "- ~24h 45min for 22,275,139 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90726c88-171b-4227-89ee-f14b33abacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_all = pd.read_pickle(os.path.join(new_path,'_data','bot_all.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d7efd-2460-40dc-be10-5f930d8b3ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = pipe(list(bot_all['t2_text_processed']), batch_size=256, **{'truncation':True,'max_length':128})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa111d6-ea21-4392-827a-044056cb4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_all['predicted_label'] = [1 if x['label']=='LABEL_1' else 0 for x in model_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696047f7-71a4-4dba-86e3-73521da146d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_all = bot_all.astype({'t1_author_username': 'category', 't2_id': int, 't2_author_username': 'category', 'predicted_label': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83e2f7-f29b-4448-89a3-a220c5f3fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_all.to_pickle(os.path.join(new_path,'_data','bot_all.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d675790-5e3c-410a-944c-b8b11becf69d",
   "metadata": {},
   "source": [
    "### extract bot_general from bot_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e473668-22b6-4b66-ae6f-9741732d27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_all = pd.read_pickle(os.path.join(new_path,'_data','bot_all.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b313c-d020-423e-8eb6-62744c3f6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_general = bot_all[bot_all['predicted_label']==1]\n",
    "bot_general = bot_general.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39103e-f5d4-4dfa-83f8-c9c77927cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bot_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd322716-e53c-447e-a450-e25326370fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_general.to_pickle(os.path.join(new_path,'_data','bot_general.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ab031-2332-4b44-823b-8824386d208b",
   "metadata": {},
   "source": [
    "### augment bot_direct with info on accusers and accusations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe7ab0-1d37-48f1-aeb7-e470a15f7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_accusations(pd_):\n",
    "    n_accusations = [[id_,n_] for id_,n_ in pd_.groupby('t1_author_id').size().items()]\n",
    "    pd_accusations = pd.DataFrame(n_accusations, columns=['t1_author_id','n_accusations'])\n",
    "    pd_ = pd_.merge(pd_accusations, on='t1_author_id', how='left')\n",
    "    return pd_\n",
    "\n",
    "def get_n_accusers(pd_):\n",
    "    pd_lite = pd_[['t1_author_id','t2_author_id']]\n",
    "    def get_accusers(row):\n",
    "        return len(set(list(pd_lite[pd_lite['t1_author_id']==row['t1_author_id']]['t2_author_id'])))\n",
    "    pd_lite['n_accusers'] = pd_lite.apply(get_accusers, axis=1)\n",
    "    pd_lite = pd_lite.drop('t2_author_id', axis=1)\n",
    "    pd_['n_accusers'] = pd_lite['n_accusers']\n",
    "    return pd_\n",
    "\n",
    "def get_n_accusing(pd_):\n",
    "    n_accusing = [[id_,n_] for id_,n_ in pd_.groupby('t2_author_id').size().items()]\n",
    "    pd_temp = pd.DataFrame(n_accusing, columns=['t2_author_id','n_accusing'])\n",
    "    pd_ = pd_.merge(pd_temp, on='t2_author_id', how='left')\n",
    "    return pd_\n",
    "\n",
    "def get_n_accusees(pd_):\n",
    "    pd_lite = pd_[['t1_author_id','t2_author_id']]\n",
    "\n",
    "    filter_1 = pd_[pd_['n_accusing']>1]\n",
    "    filter_2 = filter_1.drop_duplicates('t1_author_id')\n",
    "    query_list = filter_2[['t1_author_id','t2_author_id']]\n",
    "    query_list = query_list.drop_duplicates('t2_author_id')\n",
    "\n",
    "    def get_accusees(row):\n",
    "        return len(set(list(pd_lite[(pd_lite['t2_author_id']==row['t2_author_id'])&(~pd_lite['t1_author_id'].isnull())]['t1_author_id'])))\n",
    "\n",
    "    query_list['n_accusees'] = query_list.apply(get_accusees, axis=1)\n",
    "    query_list = query_list.drop('t1_author_id', axis=1)\n",
    "    pd_ = pd_.merge(query_list, how='left', on='t2_author_id')\n",
    "    return pd_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4cb358-17e0-4144-b9b3-742f77a204f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bot_direct = pd.read_pickle(os.path.join(new_path,'_data','bot_direct.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d6484-d94f-4af0-956b-9c9860b299c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_direct = get_n_accusations(bot_direct)\n",
    "bot_direct = get_n_accusers(bot_direct)\n",
    "bot_direct = get_n_accusing(bot_direct)\n",
    "bot_direct = get_n_accusees(bot_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c62b96-e024-4ef6-98ae-f8282ef65c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bot_direct.to_pickle(os.path.join(new_path,'_data','bot_direct.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
